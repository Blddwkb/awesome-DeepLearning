# 深度学习发展历史

    深度学习的发展经过了半个多世纪才逐渐成熟，其历程可以分为以下几个阶段。

* 由神经科学家麦卡洛克(W.S.McCilloch) 和数学家皮兹（W.Pitts）在《数学生物物理学公告》上发表论文《神经活动中内在思想的逻辑演算》（A Logical Calculus of the Ideas Immanent in Nervous Activity）。建立了神经网络和数学模型，称为MCP模型。所谓MCP模型，其实是按照生物神经元的结构和工作原理构造出来的一个抽象和简化了的模型，也就诞生了所谓的“模拟大脑”，人工神经网络的大门由此开启。

* 计算机科学家罗森布拉特（ Rosenblatt）提出了两层神经元组成的神经网络，称之为“感知器”(Perceptrons)。第一次将MCP用于机器学习（machine learning）分类(classification)。“感知器”算法算法使用MCP模型对输入的多维数据进行二分类，且能够使用梯度下降法从训练样本中自动学习更新权值。1962年,该方法被证明为能够收敛，理论与实践效果引起第一次神经网络的浪潮。

* 纵观科学发展史，无疑都是充满曲折的，深度学习也毫不例外。 1969年，美国数学家及人工智能先驱 Marvin Minsky 在其著作中证明了感知器本质上是一种线性模型（linear model），只能处理线性分类问题，就连最简单的XOR（亦或）问题都无法正确分类。这等于直接宣判了感知器的死刑，神经网络的研究也陷入了将近20年的停滞。

* 由神经网络之父 Geoffrey Hinton 在1986年发明了适用于多层感知器（MLP）的BP（Backpropagation）算法，并采用Sigmoid进行非线性映射，有效解决了非线性分类和学习的问题。该方法引起了神经网络的第二次热潮。注：Sigmoid 函数是一个在生物学中常见的S型的函数，也称为S型生长曲线。在信息科学中，由于其单增以及反函数单增等性质，Sigmoid函数常被用作神经网络的阈值函数，将变量映射到0,1之间。

* 而到了21世纪之后，人们发现神经网络在图像以及自然语言处理等方面有很大的优势，表现良好，同时近些年来数据的激增以及硬件设备的快速发展，因此神经网络模型得到了很好的发展，并称为“深度学习”，从此深度学习进入了新的时代，成为了最热门的领域。

# 人工智能、机器学习、深度学习有什么区别和联系？

    要解释这三者之间的关系和应用，最简单的方法就是画一个同心圆，如下图，人工智能是最早出现的，也是最大、最外侧的同心圆；其次是机器学习，稍晚一点；最内侧，是深度学习，也是当今人工智能大爆炸的核心驱动。简单来说，人工智能>机器学习>深度学习。

* 从概念上来说，人工智能、机器学习和深度学习是逐渐由大到小的关系，人工智能的概念最为广泛，但一般使用这个概念的时候比较宽泛，不涉及实际的应用。机器学习是目前实现人工智能一种比较有效的方法。深度学习是目前机器学习种最热门的分支，近些年机器学习的发展主要集中在深度学习领域。

  人工智能（Artificial Intelligence）――为机器赋予人的智能
  
* 人工智能只是说明了模拟、延伸扩展人的智能的理论办法与技术的新技术，只是定义目标，并未限定方法，因此存在很多方法与分支。

  机器学习―― 一种实现人工智能的方法

* 机器学习是由明确指代的学科，研究计算机如何模拟实现人类的学习行为，其实现主要分为两步：训练和预测，即归纳和演绎，通过具体的案例中抽象出一般的规律，然后进行预测，如果和实际一致，就说明模型是有效的。目前的模型主要包含三个部分：模型假设、评价函数、优化算法。模型假设决定了前提，即假定了一个模型可能的关系，评价函数用于评价一个数据X~Y预测结果两者关系的好坏，可以衡量关系是否能好的拟合现有的观测样本，将拟合的误差最小作为优化的目标。在假设的范围内找到使得评价指标尽可能的最优的关系，这个寻找的方法就是优化算法。

* 机器学习直接来源于早期的人工智能领域。传统算法包括决策树学习、推导逻辑规划、聚类、强化学习和贝叶斯网络等等。众所周知，我们现在还没有实现强人工智能，而早期机器学习方法甚至连弱人工智能都无法实现。

  深度学习――一种实现机器学习的技术

* 深度学习与传统的机器学习算法在理论结构上是一致的，差别在于假设的复杂度。深度学习模型是输入到输出的映射函数，足够深的神经网络可以拟合任意复杂的函数。

* 人工神经网络（Artificial Neural Networks）是早期机器学习中的一个重要的算法，历经了数十年的推演。神经网络的原理是受我们大脑的生理结构――互相交叉相连的神经元启发。但与大脑中一个神经元可以连接一定距离内的任意神经元不同，人工神经网络具有离散的层、连接和数据传播的方向。

# 神经元、单层感知机、多层感知机

* 神经元细胞，是神经系统最基本的结构和功能单位。 分为细胞体和突起两部分。 细胞体由细胞核、细胞膜、细胞质组成，具有联络和整合输入信息并传出信息的作用。 突起有树突和轴突两种，这是深度学习的基础。

* 对生物的神经元细胞的结构进行模拟，则得到了一个类似神经元的运算模型，即M-P模型。

* 结合M-P模型和Hebb学习规则，感知机出现了。单层感知机和多层感知机是最基础的神经网络结构，属于前馈型网络，单层感知机是二分类的线性分类模型，输入是被感知数据集的特征向量，输出时数据集的类别{+1,-1}。单层感知机的函数近似非常有限，其决策边界必须是一个超平面，严格要求数据是线性可分的。支持向量机，用核函数修正了感知器的不足，将特征向量有效的映射到更高维的空间使得样本成为线性可分的数据集。

* 多层感知机在单层神经网络的基础上引入了多个隐藏层(hidden layer)。隐藏层位于输入层和输出层之间。

# 什么是前向传播

* 前向传播就是将上一层的输出作为下一层的输入，经过线性求和之后经过$\sigma$激活函数，并计算下一层的输出，一直到运算到输出层为止，用公式来表示即为如下的过程。 
$$ z^{(l)}=W^{(l-1)}+b^{(l)} $$

$$ a^{(l)}=\sigma(z^{(l)}) $$

# 什么是反向传播

* 反向传播（back propagation, BP）算法是 "误差反向传播" 的简称，也称为BP，允许来自代价函数的信息通过网络向后流动，以便计算梯度。

* 反向传播是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法。该方法对网络中所有权重计算损失函数的梯度。这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数。